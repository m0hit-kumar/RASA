{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m0hit-kumar/RASA/blob/main/RASA_method_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRI21ALZ101o",
        "outputId": "0534391e-b771-484c-b505-7c66b0f23b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.zip  sample_data\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/BombBlast.json     \n",
            "  inflating: data/Earthquake.json    \n",
            "  inflating: data/falseData.json     \n",
            "  inflating: data/first_data.json    \n",
            "  inflating: data/floods.json        \n",
            "  inflating: data/Tornado.json       \n",
            "  inflating: data/Tsunami.json       \n",
            "  inflating: data/Volcano.json       \n",
            "  inflating: data/War.json           \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Earthquake.json',\n",
              " 'falseData.json',\n",
              " 'War.json',\n",
              " 'first_data.json',\n",
              " 'BombBlast.json',\n",
              " 'Tsunami.json',\n",
              " 'Volcano.json',\n",
              " 'Tornado.json',\n",
              " 'floods.json']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!ls\n",
        "!unzip data.zip\n",
        "import os\n",
        "os.listdir()\n",
        "os.chdir(\"data\")\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "\n",
        "summarizer = TextRankSummarizer()\n",
        "import nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4kN9NptiIiY",
        "outputId": "20728552-10cb-4f10-b000-3aed19226a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/97.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.31.0)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pycountry>=18.2.23->sumy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2023.7.22)\n",
            "Building wheels for collected packages: breadability, docopt, pycountry\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21694 sha256=ffc22999bc209826c632676b2b9420830df555f2c4194fe5e644aa1259307a58\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=2324ee60b06f4e1c2016bf5ff8b50c5c043fae048eac27070595089853037631\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for pycountry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681833 sha256=90cf10a7e74acf7b73c018771aac26768feecd374fa70476220615d24a95bcbd\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/57/cc/290c5252ec97a6d78d36479a3c5e5ecc76318afcb241ad9dbe\n",
            "Successfully built breadability docopt pycountry\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-22.3.5 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWlkyi0diNwE",
        "outputId": "24f5a252-da19-46e9-ec45-412880033adf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_d2WEaj4wz8"
      },
      "outputs": [],
      "source": [
        "#step 1 reading files and vonverting txt to json\n",
        "import json\n",
        "\n",
        "def read_json(filename):\n",
        "  with open(filename+'.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFEfOMvs0IKQ"
      },
      "outputs": [],
      "source": [
        "#step 2 summarizing text\n",
        "test_data = \"\"\"\n",
        "    The United States of America, commonly known as the United States (U.S. or US), is a country primarily located\n",
        "    in North America. It consists of 50 states, a federal district, five major unincorporated territories, 186\n",
        "    Indian reservations, and some minor possessions. The capital city is Washington, D.C., and the most populous\n",
        "    city is New York City.\n",
        "\"\"\"\n",
        "\n",
        "def my_text_summary(text):\n",
        "  parser = PlaintextParser.from_string(text,Tokenizer(\"english\"))\n",
        "  summary =summarizer(parser.document,2)\n",
        "  text_summary=\"\"\n",
        "  for sentence in summary:\n",
        "    text_summary+=str(sentence)\n",
        "  return text_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sccYH13391K-"
      },
      "outputs": [],
      "source": [
        "#step 3 converting into rasa data convtor format\n",
        "def data_to_json(key,data_list):\n",
        "  data=[]\n",
        "  for i in data_list:\n",
        "    x={\n",
        "          \"text\": i,\n",
        "          \"intent\": key\n",
        "    }\n",
        "    data.append(x)\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht0lrUQBApjO"
      },
      "outputs": [],
      "source": [
        "#step 4  Write json file data file\n",
        "import yaml\n",
        "def write_into_json(full_data):\n",
        "  json_data = {\n",
        "    \"rasa_nlu_data\": {\n",
        "    \"common_examples\": full_data\n",
        "    }\n",
        "  }\n",
        "  json_string = json.dumps(json_data, indent=4)\n",
        "  with open(\"processedData\", \"w\") as output_file:\n",
        "    output_file.write(json_string)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5biKGhVGByUx"
      },
      "outputs": [],
      "source": [
        "#pro function\n",
        "def pro_function(filename):\n",
        "  json_data=read_json(filename)\n",
        "  news=[i[\"text\"] for i in json_data[\"news\"]]\n",
        "  summarized_news=[my_text_summary(i) for i in news]\n",
        "  result= json_summarized_data=data_to_json(filename,summarized_news)\n",
        "  return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eooBskCuHL-k"
      },
      "outputs": [],
      "source": [
        "#pro max function\n",
        "full_data_variable_list=[]\n",
        "def pro_max_function(file_list):\n",
        "  full_data_variable_list=[pro_function(i) for i in file_list]\n",
        "  full_data= [item for sublist in full_data_variable_list for item in sublist]\n",
        "  write_into_json(full_data)\n",
        "  print(\"file genarted go to into data folder and processedData is your genrated file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5g-hFXqeFZ6_"
      },
      "outputs": [],
      "source": [
        "file_list=[\"War\",\"Tornado\",\"BombBlast\",\"falseData\",\"Volcano\",\"floods\",\"Earthquake\",\"Tsunami\"] #change with respect to file name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YQJnEfaLM1o",
        "outputId": "bce01dc3-d8b7-436a-8375-d34f7b9a952a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file genarted go to into data processedData is your genrated file\n"
          ]
        }
      ],
      "source": [
        "pro_max_function(file_list)   #excute to destroy half of the data chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nvBEM1ESxno"
      },
      "outputs": [],
      "source": [
        "#step to use file\n",
        "# 1) go to pc create env\n",
        "# 2) activate env\n",
        "# 3) install rasa <pip install rasa>\n",
        "# 4) run command in root directory <rasa init>\n",
        "# 5) rasa data convert <rasa data convert nlu --data processedData.json>    note: run command in the folder where u have data otherwise change the path accordingly\n",
        "# 6) rename the generated file to nlu.yaml and place it in the data folder\n",
        "# 7) run command rasa train\n",
        "# 8) wala! you gets error. if not your model magicallly appread in models folder\n",
        "# 9) then delete the model and get some coffe/tea acc. to prefrece\n",
        "# 10) get a good life what are u doing, sab doglapan hai\n",
        "# 11) if you are smart and didnt deleted the model you can <run rasa> in terminal to test the model\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFvdmJtxf24GrHVfxr7NdE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
