{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrunIeVdR0Tf2tPiQ8N/lO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m0hit-kumar/RASA/blob/main/CollabNotebook/RASA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtV-ESeQzhkG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd36a312-3abd-4b48-ad9e-c5dd2df6195f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"facebook/bart-base\"\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "IWcvncIqz_gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRI21ALZ101o",
        "outputId": "a573a048-58a7-4ae1-da63-22e8582d4ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " BombBlast.txt\t earthquake.txt   first_data.txt    sample_data   Volcano.txt\n",
            " Downloads.zip\t falseData.txt\t 'processed data'   Tornado.txt   War.txt\n",
            "Archive:  Downloads.zip\n",
            "replace War.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#step 1 reading files and vonverting txt to json\n",
        "import json\n",
        "def txt_to_json(filename):\n",
        "  with open(filename+\".txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    json_text = file.read()\n",
        "  json_data = json.loads(json_text)\n",
        "  return json_data"
      ],
      "metadata": {
        "id": "x_d2WEaj4wz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 2 summarizing text\n",
        "test_data = \"\"\"\n",
        "    The United States of America, commonly known as the United States (U.S. or US), is a country primarily located\n",
        "    in North America. It consists of 50 states, a federal district, five major unincorporated territories, 186\n",
        "    Indian reservations, and some minor possessions. The capital city is Washington, D.C., and the most populous\n",
        "    city is New York City.\n",
        "\"\"\"\n",
        "\n",
        "def summarize(input_text):\n",
        "  inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "  summary_ids = model.generate(inputs[\"input_ids\"], max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "  summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "  return summary\n"
      ],
      "metadata": {
        "id": "MFEfOMvs0IKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def summarize_text(texts, model, tokenizer, max_length=150, num_beams=4, length_penalty=2.0):\n",
        "    # Tokenize and encode the texts in batches\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length, verbose=False)\n",
        "\n",
        "    # Generate summaries\n",
        "    summaries = []\n",
        "    for i in range(len(texts)):\n",
        "        input_ids = inputs[\"input_ids\"][i].unsqueeze(0).to(model.device)\n",
        "        attention_mask = inputs[\"attention_mask\"][i].unsqueeze(0).to(model.device)\n",
        "\n",
        "        summary_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_length,\n",
        "            min_length=10,\n",
        "            length_penalty=length_penalty,\n",
        "            num_beams=num_beams,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        summaries.append(summary)\n",
        "\n",
        "    return summaries\n"
      ],
      "metadata": {
        "id": "EfqvJBZn0BcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 3 converting into rasa data convtor format\n",
        "def data_to_json(key,data_list):\n",
        "  data=[]\n",
        "  for i in data_list:\n",
        "    x={\n",
        "          \"text\": i,\n",
        "          \"intent\": key\n",
        "    }\n",
        "    data.append(x)\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "sccYH13391K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 4  Write json file data file\n",
        "import yaml\n",
        "def write_into_json(full_data):\n",
        "  json_data = {\n",
        "    \"rasa_nlu_data\": {\n",
        "    \"common_examples\": full_data\n",
        "    }\n",
        "  }\n",
        "  json_string = json.dumps(json_data, indent=4)\n",
        "  with open(\"processedData\", \"w\") as output_file:\n",
        "    output_file.write(json_string)\n"
      ],
      "metadata": {
        "id": "Ht0lrUQBApjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pro function\n",
        "\n",
        "def pro_function(filename):\n",
        "  json_data=txt_to_json(filename)\n",
        "  news=[i[\"text\"] for i in json_data[\"news\"]]\n",
        "  # summarized_news=[summarize(i) for i in news]\n",
        "  summarized_news=summarize_text(news,model, tokenizer)\n",
        "  json_summarized_data=data_to_json(filename,summarized_news)\n"
      ],
      "metadata": {
        "id": "5biKGhVGByUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pro max function\n",
        "def pro_max_function(file_list):\n",
        "  full_data_variable_list=[pro_function(i) for i in file_list]\n",
        "  full_data= [item for sublist in full_data_variable_list for item in sublist]\n",
        "  write_into_json(full_data)"
      ],
      "metadata": {
        "id": "eooBskCuHL-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list=[\"War\",\"Tornado\",\"BombBlast\",\"falseData\",\"Volcano\"] #change with respect to file name"
      ],
      "metadata": {
        "id": "5g-hFXqeFZ6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pro_max_function(file_list)   #excute to destroy half of the data chunk"
      ],
      "metadata": {
        "id": "_YQJnEfaLM1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step to use file\n",
        "# 1) go to pc create env\n",
        "# 2) activate env\n",
        "# 3) install rasa <pip install rasa>\n",
        "# 4) run command in root directory <rasa init>\n",
        "# 5) rasa data convert <rasa data convert nlu --data processedData.json>    note: run command in the folder where u have data otherwise change the path accordingly\n",
        "# 6) rename the generated file to nlu.yaml and place it in the data folder\n",
        "# 7) run command rasa train\n",
        "# 8) wala! you gets error. if not your model magicallly appread in models folder\n",
        "# 9) then delete the model and get some coffe/tea acc. to prefrece\n",
        "# 10) get a good life what are u doing, sab doglapan hai\n",
        "# 11) if you are smart and didnt deleted the model you can <run rasa> in terminal to test the model\n"
      ],
      "metadata": {
        "id": "8nvBEM1ESxno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_lfIp3Y2bN8z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}