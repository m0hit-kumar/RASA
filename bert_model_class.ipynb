{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOdUYdrBkBO8sMvPFWykok",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m0hit-kumar/RASA/blob/main/bert_model_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpC08-RHsvHF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9MYKu1TsvHH"
      },
      "outputs": [],
      "source": [
        "data=pd.read_json(\"final.json\")\n",
        "# data2=pd.read_json(\"definitions.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3103e6b6-f91c-490a-9b62-324d225b3b54",
        "id": "kA08QMedsvHJ"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Tornado', 'volcano', 'misc', 'Earthquake', 'floods', 'War',\n",
              "       'BombBlast'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data[\"intent\"].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa9b3669-750a-4c3c-8f5f-8ec37498f1f1",
        "id": "EXevbMILsvHd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "War           2181\n",
              "BombBlast     1924\n",
              "floods        1433\n",
              "Earthquake    1411\n",
              "volcano       1369\n",
              "Tornado       1339\n",
              "misc           216\n",
              "Name: intent, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data['intent'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94683176-0cdd-43f9-e174-be485adaf13f",
        "id": "x0_FhSlisvHh"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "data=data.to_dict(orient='records')\n",
        "type(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c7325e5-8736-4fa8-8ed0-85b30542dd6a",
        "id": "P7JgEdB7svHj"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords') # for removing useless data\n",
        "nltk.download('punkt') # use unsupervised algorithm to build a model for abbreviation words, collocations, and words\n",
        "nltk.download('wordnet') #used  in information systems, word-sense disambiguation, information retrieval, automatic text classification, summarization, machine translation.\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oERzy11bsvHm"
      },
      "outputs": [],
      "source": [
        "# clean the function will apply the above mentioned preprocessing to the resumes\n",
        "def clean(text):\n",
        "    wn = nltk.WordNetLemmatizer()\n",
        "    stopword = nltk.corpus.stopwords.words('english')\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    lower = [word.lower() for word in tokens]\n",
        "    no_stopwords = [word for word in lower if word not in stopword]\n",
        "    no_alpha = [word for word in no_stopwords if word.isalpha()]\n",
        "    lemm_text = [wn.lemmatize(word) for word in no_alpha]\n",
        "    clean_text = lemm_text\n",
        "    # make list of text into string\n",
        "    result=\" \".join(clean_text)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04cc085-d052-4ed3-9178-b7f0207520bd",
        "id": "QydDAW0csvIh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJad3M92svIl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsH_UAu4svIm"
      },
      "outputs": [],
      "source": [
        " # Example data\n",
        "# data = [\n",
        "#     {\"text\": \"Floods are hitting the coastal areas\", \"intent\": \"flood\"},\n",
        "#     {\"text\": \"Tornado warning issued for the city\", \"intent\": \"tornado\"},\n",
        "#     {\"text\": \"Earthquake devastates the region\", \"intent\": \"earthquake\"},\n",
        "#     # more data here\n",
        "# ]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "156c01d68b344d7fbba0966d275cb826",
            "3558cc41f56341d49e1bead372045bcf",
            "787ef30227f0462d9b0cb13155b415cf",
            "4dc5f2e6e38d4564acbd052aa7d890a8",
            "e863be23fae8435695fadf4ec027ec32",
            "ede5fc5d3b3b4c668a252352e3178fb5",
            "a197742288714a4fb6c25db81055bae5",
            "f66cc638bb7c46488e743bbab4dcac09",
            "5cd99ad21519491aa00a2471e76b0ee7",
            "9a61278ed3fa4d77a1e5e329de1133d2",
            "95605debea204f85b618df7e3a6f2816",
            "446d252038d84dd5b3f666283f09d33f",
            "28d9bba3e12d41059d5a11a211b1f942",
            "d0a3fa130cf74f4885707f84f274b80c",
            "6e86cc85e6404f818fcd216095cde49b",
            "307239d1f9c848a68166718010d9e77e",
            "77358d612d244aa280c92c6b84ebcf6c",
            "fa18ef22f2d44afdb76a70d40a8f1398",
            "98bc9884517b423384f164f738cfb4a3",
            "43dde43f3b80480087105129f2621bfd",
            "4497fec11ce4409386a4a9204fa355b8",
            "a9da7e66ce2e417a96e2d9665021213d",
            "88d20f4fb54a4051b2bbf0b65a4279db",
            "111db2679b474f4087c7a3b31c2c2525",
            "a050eb464724486d91f602808d5db226",
            "64d6e314c0934dec9d697f1bf3d12dee",
            "a28cd64e26764dd191b60762f77ba390",
            "0e001746e58a484db623963007abb44c",
            "af1debe80dd840ae8359346e1393b383",
            "7301431246aa46b09bd06a09f2f54279",
            "5850b942b03245beaf1d8c92af2f2b6b",
            "fb0776acc08f4a6eba5754019c0c2b12",
            "03f0e4cc72434493bffc96b4b8913e3b",
            "eb9e9cc5d4fc48558e3512e5b7fa5ab5",
            "d8689910f5ba47e7b55d1e9a43dbb1cd",
            "5a4a7b10f2284b6aa84a2a436f44f695",
            "f72cc377d08544d385accac9cf54d046",
            "be2a564eab17413faed8fb283e7b8fa1",
            "43b492f95f0847ffbeb1841b6a2e11fc",
            "da9b05fa7b4d40058ffdf880edd09ef3",
            "9d9244c75212473291eb28a889fe9b1d",
            "016edf56f3164210a5a15732fcc5c84c",
            "78a311837215432290041cd011213b24",
            "028634cb6844467fa6969de7cf7bc154",
            "d18b861e6e46495bbb08a96faf009f0a",
            "7cc4ef0fccee442a8110d52e5c069746",
            "1c82934d8517458593d8d6158ceca1e2",
            "ae8e7671e6354e1b8c482543ddc6bda3",
            "290bedae6721431fa0ab85534b53308a",
            "d5ebfc4e53ee4bf99cf3cacbab40d77e",
            "d360af4802ef479e9ff5ab2713b272a7",
            "aadd0cf15ea54fd79b95f943a2286744",
            "f23c23977378426abcfefbdc2da0bd97",
            "fd136fb57b4848968adfb02db404ad47",
            "98050387c3e8465e83c6609742c40340"
          ]
        },
        "outputId": "f273c36d-a2c0-4ccb-b7de-310614685920",
        "id": "FExe6oPJsvIp"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "156c01d68b344d7fbba0966d275cb826"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "446d252038d84dd5b3f666283f09d33f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88d20f4fb54a4051b2bbf0b65a4279db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb9e9cc5d4fc48558e3512e5b7fa5ab5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d18b861e6e46495bbb08a96faf009f0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqUg4nxHsvIr"
      },
      "outputs": [],
      "source": [
        "intent_label_map={'Tornado':0, 'misc':1, 'BombBlast':2, 'War':3, 'Earthquake':4, 'floods':5,'volcano':6}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data\n",
        "def tokenize_data(data):\n",
        "    inputs = tokenizer(data[\"text\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    labels = torch.tensor([intent_label_map[data[\"intent\"]]])  # Convert intent string to numerical label\n",
        "    return inputs, labels"
      ],
      "metadata": {
        "id": "RnrMDh4msvIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_data_tokenized = [tokenize_data(d) for d in train_data]\n",
        "test_data_tokenized = [tokenize_data(d) for d in test_data]"
      ],
      "metadata": {
        "id": "pa1x-X7HsvIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_data))\n"
      ],
      "metadata": {
        "id": "bIUH0rBjsvIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    for batch in train_data_tokenized:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**batch[0], labels=batch[1])\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()"
      ],
      "metadata": {
        "id": "R41az3PnsvIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_data_tokenized:\n",
        "        outputs = model(**batch[0])\n",
        "        predicted = torch.argmax(outputs.logits, dim=1)\n",
        "        total += batch[1].size(0)\n",
        "        correct += (predicted == batch[1]).sum().item()\n"
      ],
      "metadata": {
        "id": "MufqDGn0svIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a98a47fa-f089-4432-c582-b67cc0ce9662",
        "id": "L7O4JCsTsvIy"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Save the model\n",
        "output_dir = './model_save/'\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Model saved in {output_dir}\")\n"
      ]
    }
  ]
}